{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import ast\r\n",
    "import io\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "import tqdm\r\n",
    "import plotly.express as px\r\n",
    "from scipy.spatial.distance import cosine\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras import Model \r\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\r\n",
    "SEED = 42\r\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
    "WINDOW_SIZE = 10\r\n",
    "NUM_NS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Genre Indexing\r\n",
    "\r\n",
    "Typically, tensorflow's TextVectorization layer would be used to convert a corpus of words into indexes. However, because I am vectorizing genres--which are sometimes multiple words--I will be manually implementing this functionality below.\r\n",
    "\r\n",
    "The lists of genres will be extracted from `data/data_w_genres.csv`, as this gives a set of genres per artist; this is the only dataset that has lists of genres as an input rather than just one (or no) genres listed. These sets of genres are important as they show which genres generally \"keep the company\" of other genres (e.g. hard rock may often appear with classic rock)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          [show tunes]\n",
       "8                          [comedy rock, comic, parody]\n",
       "9     [emo rap, florida rap, sad rap, underground hi...\n",
       "10                                [dark trap, meme rap]\n",
       "12    [asian american hip hop, cali rap, west coast ...\n",
       "Name: genres, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting a Series of lists of genres to form a corpus\r\n",
    "\r\n",
    "data = pd.read_csv('data/data_w_genres.csv')\r\n",
    "corpus = data['genres']\r\n",
    "corpus = corpus[corpus != '[]']\r\n",
    "corpus = corpus.apply(ast.literal_eval)\r\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_sequences = list(corpus)\r\n",
    "flat_corpus = [item for sublist in genre_sequences for item in sublist] # list of every occurance of every genre\r\n",
    "genres = set(flat_corpus) # set of all genres present in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the model's accuracy, I'm electing to remove genres which do not appear often in the dataset. The cells below walk through a quick frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>occurances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rock</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pop</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dance pop</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rap</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hip hop</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       genre  occurances\n",
       "0       rock         611\n",
       "1        pop         593\n",
       "2  dance pop         572\n",
       "3        rap         516\n",
       "4    hip hop         507"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(flat_corpus)\r\n",
    "frequency = pd.DataFrame(df.value_counts())\r\n",
    "frequency.columns = ['occurances']\r\n",
    "frequency = frequency.reset_index()\r\n",
    "frequency.columns = ['genre', 'occurances']\r\n",
    "frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frequency['occurances'].plot.hist(bins=300, alpha=0.5, figsize=(16,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the majority of these genres only appear less than 5 times within the corpus. I don't want to filter out too many genres, but my intuition says to drop genres with fewer than 25 occurances.\r\n",
    "\r\n",
    "As seen below, if I use 25 as a threshold, this will leave me with only 18% of the original genres, but this is still 542 of the more common genres. I will see how this impacts my resulting corpus (if any lists are now empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent remaining: 18.23687752355316\n",
      "length of dataset: 542\n"
     ]
    }
   ],
   "source": [
    "mask = frequency['occurances'] > 25\r\n",
    "trim_freq = frequency[mask]\r\n",
    "print(\"percent remaining: \" + str(100*len(trim_freq)/len(frequency)))\r\n",
    "print(\"length of dataset: \"+ str(len(trim_freq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre Indexing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating manual indexes for each genre\r\n",
    "\r\n",
    "genre2int = {}\r\n",
    "\r\n",
    "genre2int[''] = 0 # manually adding padding token\r\n",
    "for i,genre in enumerate(trim_freq['genre']):\r\n",
    "    genre2int[genre] = i+1 # i+1 is required because the value `0` will be used for padding\r\n",
    "\r\n",
    "# Test\r\n",
    "genre2int['funk'] # index = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'funk'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating datastructure for easily reverse lookup of indexes\r\n",
    "\r\n",
    "int2genre = {index: genre for genre, index in genre2int.items()}\r\n",
    "\r\n",
    "# Test\r\n",
    "int2genre[16] # genre = 'funk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving 'vocabulary size', the quantity of unique genres left after filtering\r\n",
    "vocab_size = len(genre2int)\r\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[55], [144], [142, 199, 48, 138], [238, 313], [198, 250]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting squences of genres to sequences of genre indexes\r\n",
    "\r\n",
    "genre_int_sequences = []\r\n",
    "for sequence in genre_sequences:\r\n",
    "    genre_int_sequences.append([genre2int[genre] for genre in sequence if genre in list(trim_freq['genre'])])\r\n",
    "    \r\n",
    "genre_int_sequences = [sequence for sequence in genre_int_sequences if sequence != []] # filter out any empty lists\r\n",
    "genre_int_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set size is now 16029 instead of original size of 18823. This is 85.1564575253679% of the original data\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data set size is now {len(genre_int_sequences)} instead of original size of {len(corpus)}. This is {100*len(genre_int_sequences)/len(corpus)}% of the original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although over 80% of the genres were removed, 85% of the sequences still remain. I am content with these results and will keep the frequency filter of 25 occurances or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding \r\n",
    "\r\n",
    "Now we have `genre_int_seqeunces`, a list of lists where the sublists represent a sequence of genres in index form. However, for the word2vec model, we need _padded lists_; this means, regardless of how many genres are listed, each sequence will be the same length filled in with `0`s when appropriate. Normally this is handled in the TextVectorization class, however, because I am using genres (which are sometimes comprised of multiple words) instead of a strict one-word vocabulary, I am going to manually create `padded_int_sequences`, the required numpy array of genre indexes and pad these lists, using the `pad_sequences` method in the [preprocessing library](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences#used-in-the-notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] => ['show tunes', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[144   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0] => ['comic', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "[142 199  48 138   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0] => ['emo rap', 'sad rap', 'underground hip hop', 'vapor trap', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "padded_int_sequences = pad_sequences(\r\n",
    "    genre_int_sequences, \r\n",
    "    padding=\"post\",\r\n",
    "    value=0)\r\n",
    "\r\n",
    "# Test\r\n",
    "\r\n",
    "for seq in padded_int_sequences[:3]:\r\n",
    "    print(f\"{seq} => {[int2genre[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(padded_int_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three sequences above show the `0`s that pad the sequence to ensure each sequence is 20 elements long (the max number of genres listed in any given sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Training Data with Skip-grams\r\n",
    "\r\n",
    "Skip-grams are pairs of a target genre and a neighboring genre within a perscribed sampling window size. With this skip grams, we can make a list of target genres and their corresponding \"contexts\"; these contexts have both the true neighbor and false genres, so you also require a list of \"labels\" that specify which is a true neighbor for supervized training.\r\n",
    "\r\n",
    "Note, within the `generate_training_data` function, a sampling table is first built. This is to account for more common genres which may be sampled more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\r\n",
    "    '''\r\n",
    "    Generates skip-gram pairs with negative sampling for a list of sequences (int-encoded sentences) \r\n",
    "    based on window size, number of negative samples and genre-options size.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        - sequences: array of padded lists\r\n",
    "        - window_size: int, how many \"neighbors\" to the left or right of the current word used for sampling\r\n",
    "        - num_ns: int, the quantity of negative samples\r\n",
    "        - vocab_size: int, the quantity of unique indexes in the corpus\r\n",
    "        - seed: int, operation level seed\r\n",
    "    Returns:\r\n",
    "        - targets: list of genre targets\r\n",
    "        - contexts: list of genre-samples around target genre (some true neighbors, others false)\r\n",
    "        - labels: list of arrays denoting which context samples are true and which are negative\r\n",
    "    '''\r\n",
    "\r\n",
    "    # Elements of each training example are appended to these lists.\r\n",
    "    targets, contexts, labels = [], [], []\r\n",
    "\r\n",
    "    # Build the sampling table for vocab_size tokens.\r\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\r\n",
    "\r\n",
    "    # Iterate over all sequences (sentences) in dataset.\r\n",
    "    for sequence in tqdm.tqdm(sequences):\r\n",
    "\r\n",
    "        # Generate positive skip-gram pairs for a sequence (sentence).\r\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\r\n",
    "            sequence, \r\n",
    "            vocabulary_size=vocab_size,\r\n",
    "            sampling_table=sampling_table,\r\n",
    "            window_size=window_size,\r\n",
    "            negative_samples=0)\r\n",
    "\r\n",
    "        # Iterate over each positive skip-gram pair to produce training examples \r\n",
    "        # with positive context word and negative samples.\r\n",
    "        for target_word, context_word in positive_skip_grams:\r\n",
    "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\r\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\r\n",
    "                true_classes=context_class,\r\n",
    "                num_true=1, \r\n",
    "                num_sampled=num_ns, \r\n",
    "                unique=True, \r\n",
    "                range_max=vocab_size, \r\n",
    "                seed=seed, \r\n",
    "                name=\"negative_sampling\")\r\n",
    "            \r\n",
    "            negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\r\n",
    "\r\n",
    "            # Build context and label vectors (for one target word)\r\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\r\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\r\n",
    "\r\n",
    "            # Append each element from the training example to lists.\r\n",
    "            targets.append(target_word)\r\n",
    "            contexts.append(context)\r\n",
    "            labels.append(label)\r\n",
    "\r\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I should be able to use this function to construct a list of targets, contexts, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16029/16029 [00:01<00:00, 11389.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15135 15135 15135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=padded_int_sequences, \n",
    "    window_size=WINDOW_SIZE, \n",
    "    num_ns=NUM_NS, \n",
    "    vocab_size=vocab_size, \n",
    "    seed=SEED)\n",
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target  : 313\n",
      "context : tf.Tensor(\n",
      "[[238]\n",
      " [ 48]\n",
      " [  8]\n",
      " [160]], shape=(4, 1), dtype=int64)\n",
      "label   : tf.Tensor([1 0 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"target  :\", targets[0])\n",
    "print(f\"context :\", contexts[0] )\n",
    "print(f\"label   :\", labels[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Dataset for Performance\r\n",
    "\r\n",
    "We can put the lists of targets, contexts, and labels together in a formal TF dataset structure using `tf.data.Dataset` which is an object of `(target_word, context_word), (label)`.\r\n",
    "\r\n",
    "[Batching](https://stackoverflow.com/questions/41175401/what-is-a-batch-in-tensorflow), in short, is a method of training on your data faster. `batch_size` is a hyperparameter and there is no \"right\" answer of what to use. Though, larger batch sizes may lead to overfitting. \r\n",
    "\r\n",
    "[Shuffling](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) a dataset allows for better randomization within batches, The dataset is filled into a buffer of size `Buffer_size` and replaces selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required. So in this case, I think the dataset size is 15043 elements.\r\n",
    "\r\n",
    "BatchDataset shapes seem to follow example above, even without squeezing. Although, I did tweak my code by setting `drop_remainder` to False.\r\n",
    "\r\n",
    "[Caching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) simply caches the elements of the dataset into memory. cached data persists across runs.\r\n",
    "\r\n",
    "[Prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) allows for faster processing as later elements are prepared while current elements are being processed. Autotuning can be used to select the maximum number of elements that will be buffered when prefetching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((None,), (None, 4, 1)), (None, 4)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 15045\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=False)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((None,), (None, 4, 1)), (None, 4)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product between the *embeddings*, or the vectorized value, of target and context words to obtain predictions for labels and compute loss against true labels in the dataset.\n",
    "\n",
    "This Word2Vec model class is comprised of:\n",
    "* a `target_embedding` layer which looks up the embedding of the word as it appears as a target word.\n",
    "* a `context_embedding` layer which looks up the embedding of the word as it appears as a context word.\n",
    "* a `dots` layer which computes to dot product of the target and context embedding from a training pair\n",
    "* a `flatten` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = Embedding(vocab_size, \n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\", )\n",
    "    self.context_embedding = Embedding(vocab_size, \n",
    "                                       embedding_dim, \n",
    "                                       input_length=NUM_NS+1)\n",
    "    self.dots = Dot(axes=(3,2))\n",
    "    self.flatten = Flatten()\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    we = self.target_embedding(target)\n",
    "    ce = self.context_embedding(context)\n",
    "    dots = self.dots([ce, we])\n",
    "    return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64 # specifies how many dimensions will be present in the vector\r\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\r\n",
    "word2vec.compile(optimizer='adam',\r\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\r\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method of vizualizing loss and metrics during training. More on this in later revisions\r\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 1/30 [>.............................] - ETA: 0s - loss: 1.3857 - accuracy: 0.3047WARNING:tensorflow:From C:\\Users\\nanai\\anaconda3\\envs\\word2vec\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/30 [=>............................] - ETA: 5s - loss: 1.3859 - accuracy: 0.2832WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.3596s). Check your callbacks.\n",
      "30/30 [==============================] - 0s 14ms/step - loss: 1.3834 - accuracy: 0.3975\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3688 - accuracy: 0.7580\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.3336 - accuracy: 0.8534\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 0s 3ms/step - loss: 1.2588 - accuracy: 0.8613\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.1406 - accuracy: 0.8536\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.9988 - accuracy: 0.8529\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.8598 - accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.7385 - accuracy: 0.8671\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.6383 - accuracy: 0.8772\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.5575 - accuracy: 0.8867\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.8974\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.9049\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.9120\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3649 - accuracy: 0.9175\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.9207\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.9255\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.9287\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.9323\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2594 - accuracy: 0.9352\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 0.2461 - accuracy: 0.9372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2564a8a3130>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the final model\r\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model resulted in fairly good fits with embedding dimensions of 64. I experimented with fewer dimesions and even at 24 dimensions the model general achieves an accuracy of ~90%. I'm selecting 64 as these embeddings generally lead to more meaningful relations between genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting final genre vectors\r\n",
    "\r\n",
    "Finally, I need to extract the weights calculated in the Word2Vec model. I'll map these weights to genres analyzed, as the represent the vectorized values of each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = list(genre2int.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(543, 64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape # confirming qty of genres and embedding dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.05009535e-02, -3.28996241e-01,  3.14803332e-01, -3.30599904e-01,\n",
       "        2.03582540e-01,  1.98665857e-01,  6.01022132e-02, -2.06712544e-01,\n",
       "       -1.62228987e-01, -1.15187332e-01,  3.52495849e-01,  4.21664640e-02,\n",
       "       -1.14983000e-01,  3.21032315e-01,  2.54042059e-01, -2.65479386e-01,\n",
       "       -1.36538818e-01,  3.90089065e-01,  2.39746854e-01,  3.49652886e-01,\n",
       "        3.66764665e-01, -1.93446562e-01,  3.63179713e-01,  3.59331369e-01,\n",
       "        2.96258837e-01, -2.52539843e-01,  6.49798214e-02, -2.18003258e-01,\n",
       "       -1.68012694e-01, -2.76329696e-01, -5.46084680e-02, -8.75389948e-02,\n",
       "       -2.94985592e-01,  2.79863000e-01, -1.57023191e-01, -2.81234860e-01,\n",
       "       -1.14655268e-05, -3.34925383e-01, -3.26450408e-01,  1.87006727e-01,\n",
       "        3.87621552e-01,  3.58506411e-01,  8.30801576e-02, -1.69880927e-01,\n",
       "       -3.52208197e-01, -1.59003034e-01, -1.45288512e-01, -3.35467875e-01,\n",
       "        4.99246195e-02, -5.58484644e-02, -1.84966952e-01,  8.48656371e-02,\n",
       "        1.54510006e-01,  1.89165801e-01, -2.58592248e-01,  1.24297790e-01,\n",
       "        2.72976249e-01, -1.01803780e-01,  2.18162164e-01,  3.68343815e-02,\n",
       "       -2.78966129e-01,  3.85034651e-01, -3.79360616e-01, -1.02186732e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of one of the weights / vectors\r\n",
    "weights[542]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ska revival'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[542]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vectors and metadata to tsv's\r\n",
    "out_v = io.open('fit_vectors/vectors.tsv', 'w', encoding='utf-8')\r\n",
    "out_m = io.open('fit_vectors/metadata.tsv', 'w', encoding='utf-8')\r\n",
    "\r\n",
    "for index, word in enumerate(vocab):\r\n",
    "  if  index == 0: continue # skip 0, it's padding.\r\n",
    "  vec = weights[index] \r\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\r\n",
    "  out_m.write(word + \"\\n\")\r\n",
    "out_v.close()\r\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving frequency data as well\r\n",
    "frequency.to_csv('fit_vectors/frequency.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Word2Vec Documentation](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "* [Visualizer](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8060bc70f878a0741aeeb63a3b5b9a33f89baa7e845faf010d26b3a9fd3246f7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('word2vec': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}